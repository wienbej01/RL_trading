"""
Unified data loader for Polygon and Databento data formats.

This module provides a unified interface for loading historical market data
from partitioned Parquet files generated by Polygon and Databento clients.
Supports schema validation, data quality checks, and performance optimizations.
"""

import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Tuple
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..utils.logging import get_logger
from ..utils.config_loader import Settings

logger = get_logger(__name__)


class DataLoaderError(Exception):
    """Exception raised for data loading errors."""
    pass


class SchemaValidationError(DataLoaderError):
    """Exception raised for schema validation errors."""
    pass


class DataQualityError(DataLoaderError):
    """Exception raised for data quality issues."""
    pass


class UnifiedDataLoader:
    """
    Unified data loader for Polygon and Databento partitioned Parquet data.

    This class provides methods to load, validate, and process market data
    from partitioned Parquet files with support for both data providers.
    """

    # Expected schema for different data types
    EXPECTED_SCHEMAS = {
        'ohlcv': {
            'timestamp': 'datetime64[ns]',
            'open': 'float64',
            'high': 'float64',
            'low': 'float64',
            'close': 'float64',
            'volume': 'float64'
        },
        'orderbook': {
            'timestamp': 'datetime64[ns]',
            'bid_price': 'float64',
            'bid_size': 'float64',
            'ask_price': 'float64',
            'ask_size': 'float64'
        },
        'trades': {
            'timestamp': 'datetime64[ns]',
            'price': 'float64',
            'size': 'float64',
            'exchange': 'object'
        }
    }

    def __init__(self, settings: Settings, data_source: str = 'auto'):
        """
        Initialize the unified data loader.

        Args:
            settings: Configuration settings
            data_source: Data source ('polygon', 'databento', or 'auto')
        """
        self.settings = settings
        self.data_source = data_source
        self.data_dir = Path("data")

        # Cache settings
        self.cache_enabled = settings.get('data', {}).get('cache_enabled', True)
        self.cache_dir = Path(settings.get('data', {}).get('cache_dir', 'data/cache'))
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Performance settings
        self.max_workers = settings.get('data', {}).get('max_workers', 4)
        self.chunk_size = settings.get('data', {}).get('chunk_size', 100000)

        # Validation settings
        self.validation_enabled = settings.get('data', {}).get('validation_enabled', True)
        self.quality_checks_enabled = settings.get('data', {}).get('quality_checks_enabled', True)

        # Data quality thresholds
        self.max_gap_minutes = settings.get('data', {}).get('max_gap_minutes', 60)
        self.max_price_change_pct = settings.get('data', {}).get('max_price_change_pct', 0.20)
        self.min_volume_threshold = settings.get('data', {}).get('min_volume_threshold', 0)

        logger.info(f"Initialized UnifiedDataLoader for {data_source} data")

    def load_data(
        self,
        symbol: str,
        start_date: Union[str, datetime],
        end_date: Union[str, datetime],
        data_type: str = 'ohlcv',
        freq: str = '1min'
    ) -> pd.DataFrame:
        """
        Load data for a symbol within a date range.

        Args:
            symbol: Instrument symbol
            start_date: Start date
            end_date: End date
            data_type: Type of data ('ohlcv', 'orderbook', 'trades')
            freq: Resampling frequency

        Returns:
            DataFrame with loaded data
        """
        try:
            # Validate data_type parameter
            if data_type not in ['ohlcv', 'orderbook', 'trades']:
                raise DataLoaderError(f"Unsupported data type: {data_type}")

            # Convert dates to datetime if needed
            if isinstance(start_date, str):
                start_date = pd.to_datetime(start_date)
            if isinstance(end_date, str):
                end_date = pd.to_datetime(end_date)

            # Check cache first
            if self.cache_enabled:
                cached_data = self._load_from_cache(symbol, start_date, end_date, data_type, freq)
                if cached_data is not None:
                    logger.info(f"Loaded cached data for {symbol}")
                    return cached_data

            # Load from partitioned files
            data = self._load_partitioned_data(symbol, start_date, end_date, data_type)

            if data.empty:
                logger.warning(f"No data found for {symbol} between {start_date} and {end_date}")
                return data

            # Validate schema
            if self.validation_enabled:
                self._validate_schema(data, data_type)

            # Perform quality checks
            if self.quality_checks_enabled:
                data = self._perform_quality_checks(data, data_type)

            # Resample if needed
            if freq != '1s' and data_type in ['ohlcv', 'orderbook']:
                data = self._resample_data(data, freq)

            # Cache the result
            if self.cache_enabled:
                self._save_to_cache(data, symbol, start_date, end_date, data_type, freq)

            logger.info(f"Loaded {len(data)} rows of {data_type} data for {symbol}")
            return data

        except Exception as e:
            logger.error(f"Failed to load data for {symbol}: {e}")
            raise DataLoaderError(f"Data loading failed: {e}")

    def _load_partitioned_data(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        data_type: str
    ) -> pd.DataFrame:
        """
        Load data from partitioned Parquet files.

        Args:
            symbol: Instrument symbol
            start_date: Start date
            end_date: End date
            data_type: Type of data

        Returns:
            Combined DataFrame from all partitions
        """
        # Determine data directory based on source
        if self.data_source == 'auto':
            # Try to detect data source
            data_dirs = self._detect_data_directories(symbol)
        else:
            data_dirs = [self.data_dir / self.data_source / "historical"]

        all_data = []

        for data_dir in data_dirs:
            if not data_dir.exists():
                continue

            # Find partitions for the symbol
            symbol_dir = data_dir / f"symbol={symbol}"
            if not symbol_dir.exists():
                continue

            # Get all partition files within date range
            partition_files = self._get_partition_files(symbol_dir, start_date, end_date)

            if not partition_files:
                continue

            # Load data from partitions (potentially in parallel)
            if len(partition_files) > 1 and self.max_workers > 1:
                data = self._load_partitions_parallel(partition_files)
            else:
                data = self._load_partitions_sequential(partition_files)

            all_data.append(data)

        if not all_data:
            return pd.DataFrame()

        # Combine all data
        combined_data = pd.concat(all_data, ignore_index=False)

        # Sort by timestamp and remove duplicates
        if not combined_data.empty:
            combined_data = combined_data.sort_index()
            combined_data = combined_data[~combined_data.index.duplicated(keep='first')]

        return combined_data

    def _detect_data_directories(self, symbol: str) -> List[Path]:
        """Detect available data directories for the symbol."""
        directories = []

        # Check for Polygon data
        polygon_dir = self.data_dir / "polygon" / "historical"
        if polygon_dir.exists():
            symbol_dir = polygon_dir / f"symbol={symbol}"
            if symbol_dir.exists():
                directories.append(polygon_dir)

        # Check for Databento data
        databento_dir = self.data_dir / "databento" / "historical"
        if databento_dir.exists():
            symbol_dir = databento_dir / f"symbol={symbol}"
            if symbol_dir.exists():
                directories.append(databento_dir)

        return directories

    def _get_partition_files(
        self,
        symbol_dir: Path,
        start_date: datetime,
        end_date: datetime
    ) -> List[Path]:
        """Get list of partition files within date range."""
        partition_files = []

        # Iterate through year/month/day partitions
        for year_dir in symbol_dir.glob("year=*"):
            if not year_dir.is_dir():
                continue

            year = int(year_dir.name.split('=')[1])
            if year < start_date.year or year > end_date.year:
                continue

            for month_dir in year_dir.glob("month=*"):
                if not month_dir.is_dir():
                    continue

                month = int(month_dir.name.split('=')[1])
                if (year == start_date.year and month < start_date.month) or \
                   (year == end_date.year and month > end_date.month):
                    continue

                for day_dir in month_dir.glob("day=*"):
                    if not day_dir.is_dir():
                        continue

                    day = int(day_dir.name.split('=')[1])

                    # Check if this day is within range
                    partition_date = datetime(year, month, day)
                    if partition_date < start_date.replace(hour=0, minute=0, second=0, microsecond=0) or \
                       partition_date > end_date.replace(hour=23, minute=59, second=59, microsecond=999999):
                        continue

                    # Find data.parquet file
                    data_file = day_dir / "data.parquet"
                    if data_file.exists():
                        partition_files.append(data_file)

        return sorted(partition_files)

    def _load_partitions_sequential(self, partition_files: List[Path]) -> pd.DataFrame:
        """Load partition files sequentially."""
        all_data = []

        for file_path in partition_files:
            try:
                df = pd.read_parquet(file_path)
                if not df.empty:
                    # Ensure timestamp is index
                    if 'timestamp' in df.columns:
                        df.set_index('timestamp', inplace=True)
                    df.index = pd.to_datetime(df.index)
                    all_data.append(df)
            except Exception as e:
                logger.warning(f"Failed to load partition {file_path}: {e}")

        if not all_data:
            return pd.DataFrame()

        return pd.concat(all_data, ignore_index=False)

    def _load_partitions_parallel(self, partition_files: List[Path]) -> pd.DataFrame:
        """Load partition files in parallel."""
        all_data = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self._load_single_partition, file_path): file_path
                for file_path in partition_files
            }

            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    df = future.result()
                    if not df.empty:
                        all_data.append(df)
                except Exception as e:
                    logger.warning(f"Failed to load partition {file_path}: {e}")

        if not all_data:
            return pd.DataFrame()

        return pd.concat(all_data, ignore_index=False)

    def _load_single_partition(self, file_path: Path) -> pd.DataFrame:
        """Load a single partition file."""
        df = pd.read_parquet(file_path)

        if df.empty:
            return df

        # Ensure timestamp is index
        if 'timestamp' in df.columns:
            df.set_index('timestamp', inplace=True)
        df.index = pd.to_datetime(df.index)

        return df

    def _validate_schema(self, data: pd.DataFrame, data_type: str):
        """
        Validate DataFrame schema against expected format.

        Args:
            data: DataFrame to validate
            data_type: Type of data

        Raises:
            SchemaValidationError: If schema validation fails
        """
        if data_type not in self.EXPECTED_SCHEMAS:
            logger.warning(f"Unknown data type: {data_type}")
            return

        expected_schema = self.EXPECTED_SCHEMAS[data_type]
        missing_columns = []
        type_mismatches = []

        for col, expected_type in expected_schema.items():
            # Skip timestamp if it's the index
            if col == 'timestamp' and (data.index.name == 'timestamp' or isinstance(data.index, pd.DatetimeIndex)):
                continue
            if col not in data.columns:
                missing_columns.append(col)
            elif not self._check_column_type(data[col], expected_type):
                type_mismatches.append(f"{col}: expected {expected_type}, got {data[col].dtype}")

        if missing_columns:
            raise SchemaValidationError(f"Missing required columns: {missing_columns}")

        if type_mismatches:
            logger.warning(f"Type mismatches found: {type_mismatches}")
            # Try to fix type mismatches
            for mismatch in type_mismatches:
                col = mismatch.split(':')[0]
                expected_type = expected_schema[col]
                try:
                    data[col] = data[col].astype(expected_type)
                    logger.info(f"Fixed type for column {col}")
                except Exception as e:
                    logger.error(f"Failed to fix type for column {col}: {e}")

    def _check_column_type(self, series: pd.Series, expected_type: str) -> bool:
        """Check if column type matches expected type."""
        actual_type = str(series.dtype)

        # Handle common type variations
        if expected_type == 'datetime64[ns]' and 'datetime' in actual_type:
            return True
        elif expected_type in ['float64', 'int64'] and 'float' in actual_type:
            return True
        elif expected_type == actual_type:
            return True

        return False

    def _perform_quality_checks(self, data: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """
        Perform data quality checks and cleaning.

        Args:
            data: DataFrame to check
            data_type: Type of data

        Returns:
            Cleaned DataFrame
        """
        original_len = len(data)

        # Check for data gaps
        if data_type in ['ohlcv', 'orderbook']:
            data = self._check_time_gaps(data)

        # Check for price anomalies
        if data_type == 'ohlcv':
            data = self._check_price_anomalies(data)

        # Check for volume anomalies
        if 'volume' in data.columns:
            data = self._check_volume_anomalies(data)

        # Remove rows with all NaN values
        data = data.dropna(how='all')

        cleaned_len = len(data)
        if cleaned_len < original_len:
            logger.info(f"Removed {original_len - cleaned_len} rows during quality checks")

        return data

    def _check_time_gaps(self, data: pd.DataFrame) -> pd.DataFrame:
        """Check for and handle time gaps in data."""
        if data.empty:
            return data

        # Calculate time differences
        time_diffs = data.index.to_series().diff().dt.total_seconds() / 60  # in minutes

        # Find gaps larger than threshold
        large_gaps = time_diffs > self.max_gap_minutes
        gap_count = large_gaps.sum()

        if gap_count > 0:
            logger.warning(f"Found {gap_count} time gaps larger than {self.max_gap_minutes} minutes")
            # For now, just log the issue. In production, might want to interpolate or flag

        return data

    def _check_price_anomalies(self, data: pd.DataFrame) -> pd.DataFrame:
        """Check for price anomalies."""
        if 'close' not in data.columns:
            return data

        # Calculate percentage changes
        pct_changes = data['close'].pct_change().abs()

        # Find anomalies
        anomalies = pct_changes > self.max_price_change_pct
        anomaly_count = anomalies.sum()

        if anomaly_count > 0:
            logger.warning(f"Found {anomaly_count} price changes > {self.max_price_change_pct*100}%")
            # Could remove or flag anomalous rows here

        return data

    def _check_volume_anomalies(self, data: pd.DataFrame) -> pd.DataFrame:
        """Check for volume anomalies."""
        if 'volume' not in data.columns:
            return data

        # Check for zero or negative volume
        invalid_volume = (data['volume'] <= self.min_volume_threshold)
        invalid_count = invalid_volume.sum()

        if invalid_count > 0:
            logger.warning(f"Found {invalid_count} rows with invalid volume")
            # Remove invalid volume rows
            data = data[~invalid_volume]

        return data

    def _resample_data(self, data: pd.DataFrame, freq: str) -> pd.DataFrame:
        """Resample data to specified frequency."""
        if data.empty:
            return data

        # Define aggregation functions based on available columns
        agg_funcs = {}

        if 'open' in data.columns:
            agg_funcs['open'] = 'first'
        if 'high' in data.columns:
            agg_funcs['high'] = 'max'
        if 'low' in data.columns:
            agg_funcs['low'] = 'min'
        if 'close' in data.columns:
            agg_funcs['close'] = 'last'
        if 'volume' in data.columns:
            agg_funcs['volume'] = 'sum'
        if 'bid_price' in data.columns:
            agg_funcs['bid_price'] = 'last'
        if 'ask_price' in data.columns:
            agg_funcs['ask_price'] = 'last'
        if 'bid_size' in data.columns:
            agg_funcs['bid_size'] = 'sum'
        if 'ask_size' in data.columns:
            agg_funcs['ask_size'] = 'sum'

        if agg_funcs:
            data = data.resample(freq).agg(agg_funcs)

        return data

    def _load_from_cache(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        data_type: str,
        freq: str
    ) -> Optional[pd.DataFrame]:
        """Load data from cache if available."""
        cache_key = f"{symbol}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_{data_type}_{freq}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"

        if cache_file.exists():
            try:
                data = pd.read_parquet(cache_file)
                if not data.empty:
                    # Ensure timestamp is index
                    if 'timestamp' in data.columns:
                        data.set_index('timestamp', inplace=True)
                    data.index = pd.to_datetime(data.index)
                    return data
            except Exception as e:
                logger.warning(f"Failed to load cache: {e}")

        return None

    def _save_to_cache(
        self,
        data: pd.DataFrame,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        data_type: str,
        freq: str
    ):
        """Save data to cache."""
        if data.empty:
            return

        cache_key = f"{symbol}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_{data_type}_{freq}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"

        try:
            # Reset index to save timestamp as column
            data_to_save = data.reset_index()
            data_to_save.to_parquet(cache_file)
            logger.debug(f"Cached data to {cache_file}")
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")

    def get_data_info(self, symbol: str) -> Dict[str, Any]:
        """
        Get information about available data for a symbol.

        Args:
            symbol: Instrument symbol

        Returns:
            Dictionary with data information
        """
        info = {
            'symbol': symbol,
            'data_sources': [],
            'date_range': None,
            'total_files': 0,
            'total_size_mb': 0
        }

        # Check both data sources
        for source in ['polygon', 'databento']:
            data_dir = self.data_dir / source / "historical"
            if not data_dir.exists():
                continue

            symbol_dir = data_dir / f"symbol={symbol}"
            if not symbol_dir.exists():
                continue

            info['data_sources'].append(source)

            # Count files and calculate size
            total_files = 0
            total_size = 0

            for parquet_file in symbol_dir.rglob("*.parquet"):
                total_files += 1
                total_size += parquet_file.stat().st_size

            info['total_files'] += total_files
            info['total_size_mb'] += total_size / (1024 * 1024)

            # Get date range
            dates = []
            for year_dir in symbol_dir.glob("year=*"):
                year = int(year_dir.name.split('=')[1])
                for month_dir in year_dir.glob("month=*"):
                    month = int(month_dir.name.split('=')[1])
                    for day_dir in month_dir.glob("day=*"):
                        day = int(day_dir.name.split('=')[1])
                        dates.append(datetime(year, month, day))

            if dates:
                info['date_range'] = {
                    'start': min(dates),
                    'end': max(dates)
                }

        return info

    def clear_cache(self, symbol: Optional[str] = None):
        """
        Clear data cache.

        Args:
            symbol: Optional symbol to clear cache for
        """
        if symbol:
            # Clear cache for specific symbol
            pattern = f"{symbol}_*.parquet"
            cache_files = list(self.cache_dir.glob(pattern))
            for file in cache_files:
                file.unlink()
            logger.info(f"Cleared cache for symbol: {symbol}")
        else:
            # Clear all cache
            for file in self.cache_dir.glob("*.parquet"):
                file.unlink()
            logger.info("Cleared all cache")