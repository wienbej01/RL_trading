"""
Unified data loader for Polygon and Databento data formats.

This module provides a unified interface for loading historical market data
from partitioned Parquet files generated by Polygon and Databento clients.
Supports schema validation, data quality checks, and performance optimizations.
"""

import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Tuple
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..utils.logging import get_logger
from ..utils.config_loader import Settings

logger = get_logger(__name__)


class DataLoaderError(Exception):
    """Exception raised for data loading errors."""
    pass


class SchemaValidationError(DataLoaderError):
    """Exception raised for schema validation errors."""
    pass


class DataQualityError(DataLoaderError):
    """Exception raised for data quality issues."""
    pass


class UnifiedDataLoader:
    """
    Unified data loader for Polygon and Databento partitioned Parquet data.

    This class provides methods to load, validate, and process market data
    from partitioned Parquet files with support for both data providers.
    """

    # Expected schema for different data types
    EXPECTED_SCHEMAS = {
        'ohlcv': {
            'timestamp': 'datetime64[ns]',
            'open': 'float64',
            'high': 'float64',
            'low': 'float64',
            'close': 'float64',
            'volume': 'float64'
        },
        'orderbook': {
            'timestamp': 'datetime64[ns]',
            'bid_price': 'float64',
            'bid_size': 'float64',
            'ask_price': 'float64',
            'ask_size': 'float64'
        },
        'trades': {
            'timestamp': 'datetime64[ns]',
            'price': 'float64',
            'size': 'float64',
            'exchange': 'object'
        }
    }

    def __init__(self, settings: Settings, data_source: str = 'auto'):
        """
        Initialize the unified data loader.

        Args:
            settings: Configuration settings
            data_source: Data source ('polygon', 'databento', or 'auto')
        """
        self.settings = settings
        self.data_source = data_source
        self.data_dir = Path("data")

        # Cache settings - FIXED: Use safer pattern to avoid dict as key
        data_config = settings._config.get('data', {})
        self.cache_enabled = data_config.get('cache_enabled', True) if isinstance(data_config, dict) else True
        self.cache_dir = Path(data_config.get('cache_dir', 'data/cache') if isinstance(data_config, dict) else 'data/cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Performance settings
        self.max_workers = data_config.get('max_workers', 4) if isinstance(data_config, dict) else 4
        self.chunk_size = data_config.get('chunk_size', 100000) if isinstance(data_config, dict) else 100000

        # Validation settings
        self.validation_enabled = data_config.get('validation_enabled', True) if isinstance(data_config, dict) else True
        self.quality_checks_enabled = data_config.get('quality_checks_enabled', True) if isinstance(data_config, dict) else True

        # Data quality thresholds
        self.max_gap_minutes = data_config.get('max_gap_minutes', 60) if isinstance(data_config, dict) else 60
        self.max_price_change_pct = data_config.get('max_price_change_pct', 0.20) if isinstance(data_config, dict) else 0.20
        self.min_volume_threshold = data_config.get('min_volume_threshold', 0) if isinstance(data_config, dict) else 0

        logger.info(f"Initialized UnifiedDataLoader for {data_source} data")

    # ... rest of the file remains the same ...